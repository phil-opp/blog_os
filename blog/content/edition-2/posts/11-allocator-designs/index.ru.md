+++
title = "Allocator Designs"
weight = 11
path = "ru/allocator-designs"
date = 2020-01-20

# !TODO: check links
[extra]
chapter = "Memory Management"
# Please update this when updating the translation !TODO: added based_commit
translate_based_on_commit = ""
# GitHub usernames of the people that translated this post
translators = ["TakiMoysha"]
+++

В этой статье объясняется, как реализовать аллокаторы на куче с нуля. В ней представлены и обсуждаются различные конструкции аллокаторов, включая аллокацию с перемещением, аллокацию связанных списков и аллокацию блоков фиксированного размера. Для каждой из трех конструкций мы создадим базовую реализацию, которую можно использовать для нашего ядра.

<!-- more -->

Этот блог открыто разрабатывается на [GitHub]. Если у вас есть какие-либо проблемы или вопросы, пожалуйста, создайте там issue. Вы также можете оставлять комментарии [внизу страницы]. Полный исходный код для этого поста можно найти в ветке [`post-11`][post branch].

[GitHub]: https://github.com/phil-opp/blog_os
[внизу страницы]: #comments
<!-- fix for zola anchor checker (target is in template): <a id="comments"> -->
[post branch]: https://github.com/phil-opp/blog_os/tree/post-11

<!-- toc -->

## Введение

В [предыдущем посте] мы добавили в наше ядро базовую поддержку аллокаций памяти в куче. Для этого мы [создали новый регион памяти][map-heap] в таблицах страниц и [использовали крейт `linked_list_allocator`][use-alloc-crate] для управления этой памятью. Хотя теперь у нас есть рабочая кучу, мы оставили большую часть работы крейту аллокатора, не пытаясь понять, как он работает.

[предыдущем посте]: @/edition-2/posts/10-heap-allocation/index.md
[map-heap]: @/edition-2/posts/10-heap-allocation/index.md#creating-a-kernel-heap
[use-alloc-crate]: @/edition-2/posts/10-heap-allocation/index.md#using-an-allocator-crate

В этом посте мы покажем, как создать собственный аллокатор кучи с нуля, вместо того чтобы полагаться на существующий крейт-аллокатор. Мы обсудим различные конструкции аллокаторов, включая упрощенный _bump allocator_ и базовый _fixed-size block allocator_, и воспользуемся этими знаниями для реализации аллокатора с улучшенной производительностью (по сравнению с крейтом `linked_list_allocator`).

### Design Goals

Ответственность аллокатора - управление доступной памятью в куче. Он должен возвращать неиспользуемую память при вызовах `alloc` и отслеживать память, освобожденную с помощью `dealloc`, чтобы ее можно было использовать повторно. Самое главное, он никогда не должен выделять память, которая уже используется где-то, поскольку это приведет к неопределенному поведению (undefined behavior).

Помимо корректности, существует множество второстепенных целей проектирования. Например, аллокатор должен эффективно использовать доступную память и поддерживать низкий уровень [_фрагментации_]. Кроме того, он должен хорошо работать в приложениях с распараллеливанием задач и масштабироваться до любого количества процессоров. Для максимальной производительности он может даже оптимизировать структуру памяти (memory layout) с учетом кэшей ЦП, чтобы улучшить [локальность кэша] и избежать [false sharing].

<!-- TODO: TERM: memory layout - структура памяти -->
> примечание: memory layout в русском встречается как "структура памяти", но устоявшегося термина нету, обычно пишут memory layout если это важно для контекста. Под этим понимается размер, выравнивание, начало и конец участка памяти. Описывает то, как аллокатор выделяет память.

[локальность кэша]: https://www.geeksforgeeks.org/locality-of-reference-and-cache-operation-in-cache-memory/
[_фрагментации_]: https://en.wikipedia.org/wiki/Fragmentation_(computing)
[false sharing]: https://mechanical-sympathy.blogspot.de/2011/07/false-sharing.html

Эти требования могут сделать хорошие аллокаторы очень сложными. Например, [jemalloc] имеет более 30 000 строк кода. Такая сложность часто нежелательна в коде ядра, где одна ошибка может привести к серьезным уязвимостям безопасности. К счастью, шаблоны выделения памяти в коде ядра часто гораздо проще по сравнению с userspace кодом, поэтому относительно простых аллокаторов часто достаточно.

[jemalloc]: http://jemalloc.net/

<!-- !TODO: design = архитектура -->
Ниже мы представляем три возможных архитектуры аллокатора для ядра и объясняем их преимущества и недостатки.

## Bump Allocator

Самая простая конструкция аллокатора — это _bump allocator_ (также известный как _stack allocator_). Он выделяет память линейно и отслеживает только количество выделенных байтов и количество аллокаций. Он полезен только в очень конкретных случаях, поскольку имеет серьезное ограничение: он может освободить только всю память за раз.

### Idea

Идея bump-аллокатора заключается в линейном выделении памяти через увеличение (_"bumping"_) переменной `next`, которая указывает на начало неиспользуемой памяти. В начале `next` указывает на начало кучи. При каждом выделении `next` увеличивается на размер аллокации, так что она всегда указывает на границу между использованной и неиспользованной памятью:

![Область памяти кучи в три момента времени:
 1: В начале кучи существует одна аллокация; указатель `next` указывает на его конец.
 2: Вторая аллокация была добавлена сразу после первой; указатель `next` указывает на конец второй аллокации.
 3: Третья аллокация бал добавлена сразу после второго; указатель `next` указывает на конец третьей аллокации.](bump-allocation.svg)

Указатель `next` движется только в одном направлении и поэтому никогда не выделяет одну и ту же область памяти дважды. Когда он достигает конца кучи, больше нельзя выделить память, что приводит к ошибке нехватки памяти при следующем выделении.

Bump-аллокатор часто реализуется с помощью счетчика аллокаций, который увеличивается на 1 при каждом вызове `alloc` и уменьшается на 1 при каждом вызове `dealloc`. Когда счетчик аллокаций достигает нуля, это означает, что все выделения в куче были освобождены. В этом случае указатель `next` может быть сброшен на начальный адрес кучи, так что вся память кучи снова становится доступной для выделений.

### Implementation

Мы начинаем реализацию с объявления нового подмодуля `allocator::bump`:

```rust
// src/allocator.rs

pub mod bump;
```

Содержимое подмодуля находится в новом файле `src/allocator/bump.rs`, который мы создаем с содержанием:

```rust
// src/allocator/bump.rs

pub struct BumpAllocator {
    heap_start: usize,
    heap_end: usize,
    next: usize,
    allocations: usize,
}

impl BumpAllocator {
    /// создаем новый, пустой bump-аллокатор.
    pub const fn new() -> Self {
        BumpAllocator {
            heap_start: 0,
            heap_end: 0,
            next: 0,
            allocations: 0,
        }
    }

    /// Инициализирует bump-аллокатор с заданными границами кучи.
    ///
    /// Этот метод небезопасен, поскольку вызывающая сторона должна убедиться, что заданный
    /// диапазон памяти не используется. Кроме того, этот метод должен вызываться только один раз.
    pub unsafe fn init(&mut self, heap_start: usize, heap_size: usize) {
        self.heap_start = heap_start;
        self.heap_end = heap_start + heap_size;
        self.next = heap_start;
    }
}
```

Поля `heap_start` и `heap_end` отслеживают нижнюю и верхнюю границы области памяти кучи. Вызывающая сторона должна убедиться, что эти адреса действительны, иначе аллокатор вернет недействительную память. По этой причине функция `init` должна быть помечена как `unsafe`.

Цель поля `next` — всегда указывать на первый неиспользуемый байт кучи, т. е. на начальный адрес следующего выделения. В функции `init` оно устанавливается в значение `heap_start`, поскольку в начале вся куча не используется. При каждом выделении это поле увеличивается на размер выделения (_«bumped»_), чтобы гарантировать, что мы не вернем одну и ту же область памяти дважды.

Поле `allocations` — это простой счетчик активных аллокаций, цель которого — сбросить аллокатор после освобождения последней аллокации. Оно инициализируется со значением 0.

Мы решили создать отдельную функцию `init` вместо того, чтобы выполнять инициализацию непосредственно в `new`, чтобы интерфейс оставался идентичным аллокатору, предоставляемому крейтом `linked_list_allocator`. Благодаря этому, аллокаторы можно переключать без дополнительных изменений кода.

### Implementing `GlobalAlloc`

Как [объяснялось в предыдущем посте][global-alloc], все аллокаторы кучи должны реализовывать трейт [`GlobalAlloc`], который определен следующим образом:

[global-alloc]: @/edition-2/posts/10-heap-allocation/index.md#the-allocator-interface
[`GlobalAlloc`]: https://doc.rust-lang.org/alloc/alloc/trait.GlobalAlloc.html

```rust
pub unsafe trait GlobalAlloc {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8;
    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout);

    unsafe fn alloc_zeroed(&self, layout: Layout) -> *mut u8 { ... }
    unsafe fn realloc(
        &self,
        ptr: *mut u8,
        layout: Layout,
        new_size: usize
    ) -> *mut u8 { ... }
}
```

Необходимы только методы `alloc` и `dealloc`; другие два обладают реализацией по умолчанию и могут быть опущены.

#### First Implementation Attempt

Попробуем реализовать метод `alloc` для нашего `BumpAllocator`:

```rust
// in src/allocator/bump.rs

use alloc::alloc::{GlobalAlloc, Layout};

unsafe impl GlobalAlloc for BumpAllocator {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        // TODO alignment and bounds check
        let alloc_start = self.next;
        self.next = alloc_start + layout.size();
        self.allocations += 1;
        alloc_start as *mut u8
    }

    unsafe fn dealloc(&self, _ptr: *mut u8, _layout: Layout) {
        todo!();
    }
}
```

Сначала мы используем поле `next` в качестве начального адреса для нашей аллокации. Затем мы обновляем поле `next`, чтобы оно указывало на конечный адрес аллокации, который является следующим неиспользуемым адресом в куче. Перед тем, как вернуть начальный адрес аллокации в виде указателя `*mut u8`, мы увеличиваем счетчик `allocations` на 1.

Обратите внимание, что мы не выполняем никаких проверок границ или корректировок выравнивания, поэтому эта реализация еще не является безопасной. Это не имеет большого значения, поскольку в любом случае она не компилируется с ошибкой:

```
error[E0594]: cannot assign to `self.next` which is behind a `&` reference
  --> src/allocator/bump.rs:29:9
   |
29 |         self.next = alloc_start + layout.size();
   |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ `self` is a `&` reference, so the data it refers to cannot be written
```
(Та же ошибка будет возникать для строки `self.allocations += 1`. Мы опустили это здесь для краткости.)

Ошибка возникает т.к. методы [`alloc`] и [`dealloc`] трейта `GlobalAlloc` работают только с неизменяемой ссылкой `&self`, поэтому обновление полей `next` и `allocations` невозможно. Это создает проблему, поскольку обновление `next` при каждой аллокации является основным принципом работы bump-аллокатора.

[`alloc`]: https://doc.rust-lang.org/alloc/alloc/trait.GlobalAlloc.html#tymethod.alloc
[`dealloc`]: https://doc.rust-lang.org/alloc/alloc/trait.GlobalAlloc.html#tymethod.dealloc

#### `GlobalAlloc` and Mutability

Прежде чем рассматривать возможное решение этой проблемы изменяемости, давайте попробуем понять, почему методы трейта `GlobalAlloc` определены с аргументами `&self`: как мы видели [в предыдущем посте][global-allocator], глобальный аллокатор кучи определяется добавлением атрибута `#[global_allocator]` к `static`, который реализует трейт `GlobalAlloc`. Статические переменные в Rust иммутабельны, поэтому нет возможности вызвать метод, принимающий `&mut self`, на статическом аллокаторе. По этой причине все методы `GlobalAlloc` принимают только неизменяемую ссылку `&self`.

[global-allocator]:  @/edition-2/posts/10-heap-allocation/index.md#the-global-allocator-attribute

К счастью, есть способ получить ссылку `&mut self` из ссылки `&self`: мы можем использовать синхронизированную [внутреннюю изменяемость], обернув аллокатор в спинлок [`spin::Mutex`]. Этот тип предоставляет метод `lock`, который выполняет [взаимное исключение] и, таким образом, безопасно превращает ссылку `&self` в ссылку `&mut self`. Мы уже несколько раз использовали тип оболочки в нашем ядре, например, для [текстового буфера VGA][vga-mutex].

[interior mutability]: https://doc.rust-lang.org/book/ch15-05-interior-mutability.html
[vga-mutex]: @/edition-2/posts/03-vga-text-buffer/index.md#spinlocks
[`spin::Mutex`]: https://docs.rs/spin/0.5.0/spin/struct.Mutex.html
[mutual exclusion]: https://en.wikipedia.org/wiki/Mutual_exclusion

#### A `Locked` Wrapper Type

С помощью обертки `spin::Mutex` мы может реализовать трейт `GlobalAlloc` для нашего bump-аллокатора. Фокус в том, что реализация трейта не для `BumpAllocator`, а для обернутого типа `spin::Mutex::<BumpAllocator>`:

```rust
unsafe impl GlobalAlloc for spin::Mutex<BumpAllocator> {…}
```

К сожалению, это все еще не работает, т.к. компилятор Rust не разрешает реализации трейта для типов определенных в других крейтов:

```
error[E0117]: only traits defined in the current crate can be implemented for arbitrary types
  --> src/allocator/bump.rs:28:1
   |
28 | unsafe impl GlobalAlloc for spin::Mutex<BumpAllocator> {
   | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^--------------------------
   | |                           |
   | |                           `spin::mutex::Mutex` is not defined in the current crate
   | impl doesn't use only types from inside the current crate
   |
   = note: define and implement a trait or new type instead
```

Что бы поправить это, мы должны создать нашу собственную обертку вокруг `spin::Mutex`:

```rust
// src/allocator.rs

/// Обертка вокруг spin::Mutex для доступа к реализации.
pub struct Locked<A> {
    inner: spin::Mutex<A>,
}

impl<A> Locked<A> {
    pub const fn new(inner: A) -> Self {
        Locked {
            inner: spin::Mutex::new(inner),
        }
    }

    pub fn lock(&self) -> spin::MutexGuard<A> {
        self.inner.lock()
    }
}
```

Этот тип является общей оберткой для `spin::Mutex<A>`. Он не налагает никаких ограничений на тип `A`, поэтому его можно использовать для обертки всех типов, а не только аллокаторов. Он предоставляет простую функцию-конструктор `new`, которая оболочает заданное значение. Для удобства он также предоставляет функцию `lock`, которая вызывает `lock` на обёрнутом `Mutex`. Поскольку тип `Locked` достаточно общий, чтобы быть полезным и для других реализаций аллокаторов, мы поместили его в родительский модуль `allocator`.

#### Implementation for `Locked<BumpAllocator>`

Тип `Locked` определен в нашем собственном крейте (в отличие от `spin::Mutex`), поэтому мы можем использовать его для реализации `GlobalAlloc` для нашего bump-аллокатора. Полная реализация выглядит следующим образом:

```rust
// src/allocator/bump.rs

use super::{align_up, Locked};
use alloc::alloc::{GlobalAlloc, Layout};
use core::ptr;

unsafe impl GlobalAlloc for Locked<BumpAllocator> {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        let mut bump = self.lock(); // получаем мутабельную ссылку

        let alloc_start = align_up(bump.next, layout.align());
        let alloc_end = match alloc_start.checked_add(layout.size()) {
            Some(end) => end,
            None => return ptr::null_mut(),
        };

        if alloc_end > bump.heap_end {
            ptr::null_mut() // out of memory
        } else {
            bump.next = alloc_end;
            bump.allocations += 1;
            alloc_start as *mut u8
        }
    }

    unsafe fn dealloc(&self, _ptr: *mut u8, _layout: Layout) {
        let mut bump = self.lock(); // получаем мутабельную ссылку

        bump.allocations -= 1;
        if bump.allocations == 0 {
            bump.next = bump.heap_start;
        }
    }
}
```
Первым шагом как для `alloc`, так и для `dealloc` является вызов метода [`Mutex::lock`] через поле `inner`, чтобы получить мутабельную ссылку на тип аллокатора. Экземпляр остается заблокированным до конца метода, это нужно чтобы не было race dondition многопоточных контексте (скоро мы добавим поддержку многопоточности).

[`Mutex::lock`]: https://docs.rs/spin/0.5.0/spin/struct.Mutex.html#method.lock

По сравнению с предыдущим прототипом, реализация `alloc` теперь учитывает требования к выравниванию и проверяет границы, чтобы гарантировать, что аллокации остаются в пределах области кучи. Первый шаг - округлить адрес `next` до значения выравнивания, указанного аргументом `Layout`. Код функции `align_up` будет показан чуть позже. Затем мы добавляем запрошенный размер аллокации к `alloc_start`, чтобы получить конечный адрес блока. Чтобы предотвратить переполнение integer при больших аллокациях, мы используем метод [`checked_add`]. Если происходит переполнение или если результирующий конечный адрес блока больше конечного адреса кучи, мы возвращаем нулевой указатель, указывающий на нехватку памяти. В противном случае мы обновляем адрес `next` и увеличиваем счетчик `allocations` на 1, как и раньше. Наконец, мы возвращаем адрес `alloc_start`, преобразованный в указатель `*mut u8`.

[`checked_add`]: https://doc.rust-lang.org/std/primitive.usize.html#method.checked_add
[`Layout`]: https://doc.rust-lang.org/alloc/alloc/struct.Layout.html

Функция `dealloc` игнорирует заданные аргументы указателя и `Layout`. Вместо этого она просто уменьшает счетчик `allocations`. Если счетчик снова достигает значения `0`, это означает, что все выделенные области памяти были снова освобождены. В этом случае она сбрасывает адрес `next` в адрес `heap_start`, чтобы снова сделать доступной всю память кучи.

#### Address Alignment

Функция `align_up` достаточно универсальна, чтобы мы могли поместить ее в родительский модуль `allocator`. Базовая реализация выглядит следующим образом:

```rust
// src/allocator.rs

/// выравнивание addr по align до верхнего значения
fn align_up(addr: usize, align: usize) -> usize {
    let remainder = addr % align;
    if remainder == 0 {
        addr // addr already aligned
    } else {
        addr - remainder + align
    }
}
```

Функция сначала вычисляет [remainder][remainder] от деления `addr` на `align`. Если remainder равен 0, то адрес уже выровнен по заданному align. В противном случае мы выравниваем адрес, вычитая remainder (чтобы новый remainder стал равен 0), а затем прибавляя значение align (чтобы адрес не стал меньше исходного).

[remainder]: https://en.wikipedia.org/wiki/Euclidean_division

Заметьте, это не самый эффективный способ реализации этой функции. Гораздо более быстрая реализация выглядит так:

```rust
/// выравнивание addr по align до верхнего значения.
///
/// требуется что бы `align` был кратен 2.
fn align_up(addr: usize, align: usize) -> usize {
    (addr + align - 1) & !(align - 1)
}
```

Этот метод требует, чтобы `align` было степенью двойки, что можно гарантировать с помощью трейта `GlobalAlloc` (и его параметра [`Layout`]). Это позволяет создать [bitmask] для выравнивания адреса очень эффективным способом. Чтобы понять, как это работает, давайте пройдемся по нему шаг за шагом, начиная с правой стороны:

[`Layout`]: https://doc.rust-lang.org/alloc/alloc/struct.Layout.html
[bitmask]: https://en.wikipedia.org/wiki/Mask_(computing)

- Поскольку `align` является степенью двойки, его [двоичное представление][binary representation] имеет только один установленный бит (например, `0b000100000`). Это означает, что `align - 1` имеет все нижние биты установленными (например, `0b00011111`).
- Создавая [битовое `NOT`][bitwise `NOT`] с помощью оператора `!`, мы получаем число, в котором установлены все биты, кроме битов, меньших, чем `align` (например, `0b…111111111100000`).
- Выполняя [битовое `AND`][bitwise `AND`] над адресом и `!(align - 1)`, мы выравниваем адрес _вниз_. Это работает путем очистки всех битов, которые ниже `align`.
- Поскольку мы хотим выровнять вверх, а не вниз, мы увеличиваем `addr` на `align - 1` перед выполнением битового `AND`. Таким образом, уже выровненные адреса остаются прежними, а невыровненные адреса округляются до следующей границы выравнивания.

[binary representation]: https://en.wikipedia.org/wiki/Binary_number#Representation
[bitwise `NOT`]: https://en.wikipedia.org/wiki/Bitwise_operation#NOT
[bitwise `AND`]: https://en.wikipedia.org/wiki/Bitwise_operation#AND

Какой вариант выбрать, решать вам. Оба дают одинаковый результат, только используют разные методы.

### Using It

Чтобы использовать bump-аллокатор вместо крейта `linked_list_allocator`, нам нужно обновить статическую переменную `ALLOCATOR` в файле `allocator.rs`:

```rust
// src/allocator.rs

use bump::BumpAllocator;

#[global_allocator]
static ALLOCATOR: Locked<BumpAllocator> = Locked::new(BumpAllocator::new());
```

Здесь важно, что мы объявили `BumpAllocator::new` и `Locked::new` как [`const` функции][`const` functions]. Если бы они были обычными функциями, произошла бы ошибка компиляции, поскольку выражение инициализации `static` должно быть вычислимым во время компиляции.

[`const` functions]: https://doc.rust-lang.org/reference/items/functions.html#const-functions

Нам не нужно изменять вызов `ALLOCATOR.lock().init(HEAP_START, HEAP_SIZE)` в нашей функции `init_heap`, т.к. bump-аллокатор предоставляет тот же интерфейс, что и аллокатор, из `linked_list_allocator`.

Теперь наше ядро использует наш bump-аллокатор! Все должно работать как и раньше, включая [тесты heap_allocation][`heap_allocation` tests], которые мы создали в предыдущем посте:

[`heap_allocation` tests]: @/edition-2/posts/10-heap-allocation/index.md#adding-a-test

```
> cargo test --test heap_allocation
[…]
Running 3 tests
simple_allocation... [ok]
large_vec... [ok]
many_boxes... [ok]
```

### Discussion

Большим преимуществом bump-аллокатора является ее высокая скорость. По сравнению с другими конструкциями аллокаторов (см. ниже), которые должны активно искать подходящий блок памяти и выполнять различные задачи учета в `alloc` и `dealloc`, bump-аллокатор [может быть оптимизирован][bump downwards] до нескольких ассемблерных инструкций. Это делает bump-аллокаторы полезными для оптимизации производительности аллокации, например, при создании [виртуальной библиотеки DOM][virtual DOM library].

[bump downwards]: https://fitzgeraldnick.com/2019/11/01/always-bump-downwards.html
[virtual DOM library]: https://hacks.mozilla.org/2019/03/fast-bump-allocated-virtual-doms-with-rust-and-wasm/

Хотя bump-аллокатор редко используется в качестве глобального аллокатора, принцип bump-аллокации часто применяется в форме [аллокаций в арене][arena allocation], которая в основном объединяет отдельные аллокации в пакеты для повышения производительности. Пример аллокатора арены для Rust содержится в крэте [`toolshed`].

[arena allocation]: https://mgravell.github.io/Pipelines.Sockets.Unofficial/docs/arenas.html
[`toolshed`]: https://docs.rs/toolshed/0.8.1/toolshed/index.html

#### The Drawback of a Bump Allocator

Основное ограничение bump-аллокатора это то, что он можено использовать освобожденную память только после того, как все будет освобождено. То есть одной долговечной аллокации достаточно, чтобы заблокировать повторное использование памяти. Мы можем увидеть это добавив вариацию теста `many_boxes`:

```rust
// tests/heap_allocation.rs

#[test_case]
fn many_boxes_long_lived() {
    let long_lived = Box::new(1); // new
    for i in 0..HEAP_SIZE {
        let x = Box::new(i);
        assert_eq!(*x, i);
    }
    assert_eq!(*long_lived, 1); // new
}
```

Как и тест `many_boxes`, этот тест создает большое количество аллокаций, чтобы вызвать ошибку out-of-memory, если аллокатор не переиспользует освобожденную память. Кроме того, тест создает `long_lived` аллокацию, которая существует в течение всей работы цикла.

Когда мы запускаем наш новый тест, мы видим, что он действительно завершается с ошибкой:

```
> cargo test --test heap_allocation
Running 4 tests
simple_allocation... [ok]
large_vec... [ok]
many_boxes... [ok]
many_boxes_long_lived... [failed]

Error: panicked at 'allocation error: Layout { size_: 8, align_: 8 }', src/lib.rs:86:5
```

Давайте подробно разберём, почему происходит эта ошибка. Во-первых, аллокация `long_lived` создаётся в начале кучи, тем самым увеличивая счётчик `allocations` на 1. На каждой итерации цикла создаётся короткоживущее аллокации, которые сразу же освобождаеются до начала следующей итерации. Это означает, что счётчик `allocations` временно возрастает до 2 в начале итерации и возвращается к 1 в её конце. Проблема в том, что bump-аллокатор может повторно использовать память только тогда, когда _все_ аллокации освобождены, то есть когда счётчик allocations достигает 0. Поскольку это не происходит до окончания цикла, каждая итерация выделяет новый участок памяти, что в итоге приводит к ошибке нехватки памяти после нескольких итераций.

#### Fixing the Test?

Есть два возможных трюка, которые мы могли бы использовать что бы поправить наш bump-аллокатор:

- Мы могли бы обновить `dealloc`, чтобы проверить, было ли освобожденная аллокация последней, возвращенной `alloc`, путем сравнения его конечного адреса с указателем `next`. В случае, если они равны, мы можем безопасно сбросить `next` обратно к начальному адресу освобожденной аллокации. Таким образом, каждая итерация цикла будет переиспользовать один и тот же блок памяти.
- Мы могли бы добавить метод `alloc_back`, который выделяет память с _конца_ кучи, используя дополнительное поле `next_back`. Затем мы могли бы вручную применять этот метод ко всем всех долгоживущим аллокациям, тем самым разделяя кратковременные и долговечные аллокации в куче. Заметьте, что такое разделение работает только в том случае, если заранее известно, как долго будет существовать каждая аллокация. Еще один недостаток этого подхода в том, что, что ручное управление аллокациями является трудоемким и потенциально небезопасным.

Хотя оба этих подхода позволяют пройти тест, они не являются универсальным решением, поскольку способны переиспользовать память лишь в очень конкретных случаях. Встаёт вопрос: существует ли общее решение, которое позволяет переиспользовать всю освобождённую память?

#### Reusing All Freed Memory?

Как мы узнали в [предыдущей статье][heap-intro], аллокации могут существовать произвольно долго и освобождаться в произвольном порядке. Это означает, что нам необходимо отслеживать потенциально неограниченное количество непрерывных и несвязанных участков неиспользуемой памяти, как показано в следующем примере:

[heap-intro]: @/edition-2/posts/10-heap-allocation/index.md#dynamic-memory

![](allocation-fragmentation.svg)

На графике показано состояние кучи в динамике. В начале вся куча неиспользуется, и указатель `next` равен `heap_start` (линия 1). Затем происходит первая аллокация (линия 2). На линии 3 освобождаем первый блок и аллоцируем второй. На линии 4 добавляется ещё множество блоков, половина из которых короткоживущие и уже освобождаются на линии 5, где одновременно выделяется ещё один новый блок.

Линия 5 демонстрирует фундаментальную проблему: у нас есть пять неиспользуемых областей памяти разного размера, но указатель `next` может указывать только на начало последней из них. Хотя в данном примере мы могли бы хранить начальные адреса и размеры остальных неиспользуемых регионов в массиве размером 4, это не является универсальным решением, поскольку мы легко можем создать пример с 8, 16 или 1000 неиспользуемыми обрастями памяти.

Обычно, когда у нас есть потенциально неограниченное количество элементов, мы используем коллекцию, аллоцированную в куче. Однако в нашем случае это невозможно, т.к  heap-аллокатор не может зависеть от самого себя (это привело бы к бесконечной рекурсии или взаимоблокировке(deadlock)). Поэтому нам нужно найти другое решение.

## Linked List Allocator

Одна из распространенных техник для отслеживания свободных областей памяти при реализации аллокаторов - это использование этих самых областей как хранилища. Мы используем факт, что регионы все еще маппяться в виртуальную память и храняться в на физическом фрейме, но хранящаяся информация больше не требуется. Записывая информацию об освобожденном регионе прямо в саму область, мы может отслеживать неограниченное кол-во свободных регионов без необходимости дополнительной памяти.

Наиболее частый подход к реализации - создание связанного списка (linked list) в освобожденной памяти, где каждый узел представляет собой свободную область памяти:

![](linked-list-allocation.svg)

Каждый узел списка содержит два поля: размер свободного региона и указатель на следующий свободный регион памяти. При таком подходе нам достаточно хранить лишь указатель на первый свободный регион (называемая `head`), чтобы отслеживать все свободные области, независимо от их количиства. Получившаяся структура данных часто называется [cписок свободной памяти][_free list_]

[_free list_]: https://en.wikipedia.org/wiki/Free_list

Как вы, вероятно, уже догадались по названию, именно этот метод используется крейтом `linked_list_allocator`. Аллокаторы, применяющие этот подход, также часто называются _pool-аллокаторы_ (pool allocators)

### Implementation

Далее мы создадим свой собственный простой тип `LinkedListAllocator`, который использует вышеупомянутый подход для отслеживания освобожденных областей памяти. Эта часть поста не является обязательной для будущих постов, поэтому вы можете пропустить детали реализации, если хотите.

#### The Allocator Type

Начнем с создания приватной структуры `ListNode` в новом подмодуле `allocator::linked_list`:

```rust
// src/allocator.rs

pub mod linked_list;
```

```rust
// src/allocator/linked_list.rs

struct ListNode {
    size: usize,
    next: Option<&'static mut ListNode>,
}
```

Как показано на рисунке, узел списка имеет поле `size` и опциональный указатель на следующий узел как тип `Option<&'static mut ListNode>`. Тип `&'static mut` семантически описывает [владеемый][owned] объект за указателем. По сути, это [`Box`] без деструктора, который освобождает объект в конце области видимости.

[owned]: https://doc.rust-lang.org/book/ch04-01-what-is-ownership.html
[`Box`]: https://doc.rust-lang.org/alloc/boxed/index.html

Мы реализуем следующий метод для `ListNode`:

```rust
// src/allocator/linked_list.rs

impl ListNode {
    const fn new(size: usize) -> Self {
        ListNode { size, next: None }
    }

    fn start_addr(&self) -> usize {
        self as *const Self as usize
    }

    fn end_addr(&self) -> usize {
        self.start_addr() + self.size
    }
}
```

<!-- !TODO: static linked list allocator = статический односвязный аллокатор -->
Тип имеет простую функцию-конструктор с именем `new` и методы для вычисления начального и конечного адресов представленной области. Мы делаем функцию `new` [const function], которая понадобится позже при создании статического односвязного аллокатора.

[const function]: https://doc.rust-lang.org/reference/items/functions.html#const-functions

Используя структуру `ListNode` в качестве строительного блока, мы можем создать структуру `LinkedListAllocator`:

```rust
// src/allocator/linked_list.rs

pub struct LinkedListAllocator {
    head: ListNode,
}

impl LinkedListAllocator {
    /// создаем пустой LinkedListAllocator.
    pub const fn new() -> Self {
        Self {
            head: ListNode::new(0),
        }
    }

    /// Инициализируем аллокатор с границами кучи.
    ///
    /// Эта ф-ция unsafe т.к. вызывающий должен гарантировать ,что полученные
    /// границы кучи будет корректны и куча не используется.
    /// Этот метод должен вызываться один раз
    pub unsafe fn init(&mut self, heap_start: usize, heap_size: usize) {
        unsafe {
            self.add_free_region(heap_start, heap_size);
        }
    }

    /// Добавляет полученную память к концу списка.
    unsafe fn add_free_region(&mut self, addr: usize, size: usize) {
        todo!();
    }
}
```

Структура содержит узел `head`, указывающий на первый регион кучи. Нас интересует только значение указателя `next`, поэтому в функции `ListNode::new` мы устанавливаем `size` в 0. То, что `head` является экземпляром `ListNode`, а не просто `&'static mut ListNode`, обладает плюсом в том, что реализация метода `alloc` станет проще.

Как и в случае с bump-аллокатором, функция `new` не инициализирует аллокатор с границами кучи. Помимо сохранения совместимости API, причина в том, что процедура инициализации требует записи узла в память кучи, что возможно только во время выполнения. Однако функция `new` должна быть [константой](`const` function), то есть, что бы ее можно было вычислить на этапе компиляции, т.к она будет использоваться для инициализации статической переменной `ALLOCATOR`. По этой причине мы снова предоставляем отдельный, не-константный метод `init`.


[`const` function]: https://doc.rust-lang.org/reference/items/functions.html#const-functions

Метод `init` использует метод `add_free_region`, реализация которого будет показана через мгновение. Пока что мы используем макрос [`todo!`], он укажет что реализация еще не готова и при достижении будет вызывать панику.

[`todo!`]: https://doc.rust-lang.org/core/macro.todo.html

#### The `add_free_region` Method

Метод `add_free_region` обеспечивает основную операцию _push_ в связанном списке. В настоящее время мы вызываем этот метод только из `init`, но он также будет важным методом для нашей реализации `dealloc`. Помните, что метод `dealloc` вызывается, при освобождении выделенной области памяти. Чтобы отслеживать этот освобожденный участок, мы хотим добавить его в связанный список.

Рассмотрим реализацию метода `add_free_region`:

```rust
// src/allocator/linked_list.rs

use super::align_up;
use core::mem;

impl LinkedListAllocator {
    /// Добавить полученную область памяти к концу списка.
    unsafe fn add_free_region(&mut self, addr: usize, size: usize) {
        // проверим, что освобожденная область может хранить ListNode
        assert_eq!(align_up(addr, mem::align_of::<ListNode>()), addr);
        assert!(size >= mem::size_of::<ListNode>());

        // создадим новый узел и добавим его к началу списка
        let mut node = ListNode::new(size);
        node.next = self.head.next.take();
        let node_ptr = addr as *mut ListNode;
        unsafe {
            node_ptr.write(node);
            self.head.next = Some(&mut *node_ptr)
        }
    }
}
```

Метод принимает в качестве аргумента адрес и размер области памяти и добавляет ее в начало списка. Сначала он проверяет, что данная область имеет необходимый размер и выравнена для хранения `ListNode`. Затем он создает узел и вставляет его в список, выполняя следующие шаги:

![](linked-list-allocator-push.svg)

Шаг 0 показывает состояние кучи перед вызовом `add_free_region`. На шаге 1 метод вызывается с областью памяти, помеченной на рисунке как `freed`. После начальных проверок метод создает новый `node` в своем стеке с размером освобожденной области. Затем он использует метод [`Option::take`], чтобы установить указатель `next` на текущий указатель `head`, тем самым сбрасывая указатель `head` в `None`.

[`Option::take`]: https://doc.rust-lang.org/core/option/enum.Option.html#method.take

На шаге 2 метод записывает вновь созданный `node` в начало освобожденной области памяти с помощью метода [`write`]. Затем он указатель `head` указывает на новый улез. Результирующая структура указателей выглядит немного хаотично, т.к. освобожденная область всегда вставляется в начало списка, но если мы проследим за указателями, то увидим, что каждая свободная область по-прежнему доступна из указателя `head`.

[`write`]: https://doc.rust-lang.org/std/primitive.pointer.html#method.write

#### The `find_region` Method

Вторая основная операция над связанным списком - поиск записи и ее удаление из списка. Это важная операция, необходимая для реализации метода `alloc`. Мы реализуем эту операцию в виде метода `find_region`:

```rust
// src/allocator/linked_list.rs

impl LinkedListAllocator {
    /// Смотрим свободную область заданного размера и выравнивания и
    /// удаляем ее из списка.
    ///
    /// Возвращаем кортеж из списка и начального адреса аллокации.
    fn find_region(&mut self, size: usize, align: usize)
        -> Option<(&'static mut ListNode, usize)>
    {
        // ссылка на текущий узел, обновляемая при каждой итерации
        let mut current = &mut self.head;
        // поиск достаточно большого участка памяти в связанном списке
        while let Some(ref mut region) = current.next {
            if let Ok(alloc_start) = Self::alloc_from_region(&region, size, align) {
                // область подходит для аллокации -> удалить узел из списка
                let next = region.next.take();
                let ret = Some((current.next.take().unwrap(), alloc_start));
                current.next = next;
                return ret;
            } else {
                // облать не подходит ->  перейти к следущей
                current = current.next.as_mut().unwrap();
            }
        }

        // подходящей области не найдено
        None
    }
}
```

В этом методе используется переменная `current` и цикл [`while let`] для итерации по элементам списка. В начале `current` устанавливается в (фиктивный) узел `head`. При каждой итерации он обновляется до поля `next` текущего узла (в блоке `else`). Если область подходит для выделения с заданным размером и выравниванием, она удаляется из списка и возвращается вместе с адресом `alloc_start`.

[`while let` loop]: https://doc.rust-lang.org/reference/expressions/loop-expr.html#while-let-patterns

Когда указатель `current.next` становится `None`, цикл заканчивается. Это означает, что мы прошли весь список, но не нашли подходящей области для аллокации. В этом случае мы возвращаем `None`. Подходит ли область, проверяет функция `alloc_from_region`, реализация которой будет показана чуть позже.

Давайте более подробно рассмотрим, как подходящая область удаляется из списка:

![](linked-list-allocator-remove-region.svg)

Шаг 0 показывает ситуацию до каких-либо корректировок указателей. Области `region` и `current`, а также указатели `region.next` и `current.next` отмечены на графике. На шаге 1 оба указателя `region.next` и `current.next` сбрасываются в `None` используя [`Option::take`]. Исходные указатели хранятся в локальных переменных с именами `next` и `ret`.

На шаге 2 указатель `current.next` устанавливается на локальный указатель `next`, который является исходным указателем `region.next`. В результате `current` теперь напрямую указывает на область после `region`, так что `region` больше не является элементом связанного списка. Затем функция возвращает указатель на `region`, хранящийся в локальной переменной `ret`.

##### The `alloc_from_region` Function

Функция `alloc_from_region` возвращает значение, указывающее, подходит ли область для выделения памяти заданного размера и выравнивания. Она определена следующим образом:

```rust
// src/allocator/linked_list.rs

impl LinkedListAllocator {
    /// попытка использовать регион для выделения памяти заданного размера и выравнивания.
    ///
    /// В случае успеха возвращает начальный адрес выделенной памяти.
    fn alloc_from_region(region: &ListNode, size: usize, align: usize)
        -> Result<usize, ()>
    {
        let alloc_start = align_up(region.start_addr(), align);
        let alloc_end = alloc_start.checked_add(size).ok_or(())?;

        if alloc_end > region.end_addr() {
            // region too small
            return Err(());
        }

        let excess_size = region.end_addr() - alloc_end;
        if excess_size > 0 && excess_size < mem::size_of::<ListNode>() {
            // остальная часть области слишком маленькая чтобы хранить listNode
            // это необходимо, т.к. аллокация делит область на использованную и свободную
            return Err(());
        }

        // область подходит для аллокации
        Ok(alloc_start)
    }
}
```

Сначала функция вычисляет начальный и конечный адрес потенциальной аллокации, используя функцию `align_up`, описанную выше, и метод [`checked_add`]. Если происходит переполнение или конечный адрес находится за конечным адресом области, аллокация не помещается в области, и мы возвращаем ошибку.

После этого функция выполняет менее очевидную проверку. Эта проверка необходима, поскольку в большинстве случаев выделение не вписывается в подходящую область идеально, так что часть области остается доступной для использования после аллокации. Эта часть области должна хранить свой собственный `ListNode` после выделения, поэтому она должна быть достаточно большой для этого. Проверка проверяет именно это: либо аллокация вписывается идеально (`excess_size == 0`), либо избыточный размер достаточно велик для хранения `ListNode`.

#### Implementing `GlobalAlloc`

С помощью операций, предоставляемых методами `add_free_region` и `find_region`, мы можем реализовать трейт `GlobalAlloc`. Как и в случае с bump-аллокатором, мы не реализуем trait напрямую для `LinkedListAllocator`, а только для обернутого `Locked<LinkedListAllocator>`. Обертка [`Locked`] добавляет внутреннюю мутабельность с помощью спинлока, это позволяет нам изменять сам экземпляр аллокатора, даже если методы `alloc` и `dealloc` принимают только ссылки `&self`.

[`Locked` wrapper]: @/edition-2/posts/11-allocator-designs/index.md#a-locked-wrapper-type

Реализация выглядит так:

```rust
// src/allocator/linked_list.rs

use super::Locked;
use alloc::alloc::{GlobalAlloc, Layout};
use core::ptr;

unsafe impl GlobalAlloc for Locked<LinkedListAllocator> {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        // скорректируем memory layout
        let (size, align) = LinkedListAllocator::size_align(layout);
        let mut allocator = self.lock();

        if let Some((region, alloc_start)) = allocator.find_region(size, align) {
            let alloc_end = alloc_start.checked_add(size).expect("overflow");
            let excess_size = region.end_addr() - alloc_end;
            if excess_size > 0 {
                unsafe {
                    allocator.add_free_region(alloc_end, excess_size);
                }
            }
            alloc_start as *mut u8
        } else {
            ptr::null_mut()
        }
    }

    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
        // скорректируем memory layout
        let (size, _) = LinkedListAllocator::size_align(layout);

        unsafe { self.lock().add_free_region(ptr as usize, size) }
    }
}
```

Начнём с метода `dealloc`, поскольку он проще: сначала выполняется корректировка структуры памяти (adjustment memory layout), которую мы объясним чуть позже. Затем вызываем [`Mutex::lock`] обертке [`Locked`] что бы получить ссылку на аллокатор - `&mut LinkedListAllocator`. В завершение вызываем функцию `add_free_region`, чтобы добавить освобождённую область в список свободной памяти (free list).

Метод `alloc` немного сложнее. Он начинается с тех же корректировок структуры и также вызывает [`Mutex::lock`], чтобы получить мутабельную ссылку на аллокатор. Затем он использует метод `find_region`, чтобы найти подходящую область памяти для аллокации и удалить его из списка. Если поиск возвращает None, неуспешен, метод возвращает `null_mut`, сигнализируя об ошибке - подходящей области памяти не найдено.

В случае успеха метод `find_region` возвращает кортеж из подходящей области (уже удалённого из списка) и начального адреса выделения. Используя `alloc_start`, размер аллокации и конечный адрес региона, он заново вычисляет конечный адрес области и размер излишка. Если излишек не равен нулю, вызывается `add_free_region`, чтобы вернуть оставшуюся часть региона обратно в список свободных блоков. Наконец, метод возвращает `alloc_start`, приведённый к типу `*mut u8`.


#### Layout Adjustments

Так что же это за корректировка структуры памяти (layout adjustment), которые мы делаем в начале как в `alloc`, так и в `dealloc`? Они гарантируют, что каждый выделенный блок способен хранить `ListNode`. Это важно, потому что блок памяти в какой-то момент будет освобожден, и мы хотим записать в него `ListNode`. Если блок меньше, чем `ListNode`, или не имеет правильного выравнивания, может произойти неопределенное поведение (undefined behavior).

Корректировка структуры выполняются функцией `size_align`, которая определена следующим образом:

```rust
// src/allocator/linked_list.rs

impl LinkedListAllocator {
    /// Поправить полученный layout так, что бы в выделенная область памяти
    /// также могла хранить `ListNode`.
    ///
    /// Возвращает скорректированный размер и выравнивание в виде кортежа (размер, выравнивание).
    fn size_align(layout: Layout) -> (usize, usize) {
        let layout = layout
            .align_to(mem::align_of::<ListNode>())
            .expect("adjusting alignment failed")
            .pad_to_align();
        let size = layout.size().max(mem::size_of::<ListNode>());
        (size, layout.align())
    }
}
```

Сначала функция использует метод [`align_to`] на переданном [`Layout`], чтобы при необходимости увеличить выравнивание до выравнивания `ListNode`. Затем используется метод [`pad_to_align`], чтобы округлить размер до кратного выравниванию, чтобы гарантировать, что начальный адрес следующего блока памяти также будет иметь правильное выравнивание для хранения `ListNode`.
На втором этапе она использует метод [`max`], чтобы обеспечить минимальный размер аллокации `mem::size_of::<ListNode>`. Таким образом, функция `dealloc` может безопасно записать `ListNode` в освобожденный блок памяти.

[`align_to`]: https://doc.rust-lang.org/core/alloc/struct.Layout.html#method.align_to
[`pad_to_align`]: https://doc.rust-lang.org/core/alloc/struct.Layout.html#method.pad_to_align
[`max`]: https://doc.rust-lang.org/std/cmp/trait.Ord.html#method.max

### Using it

Теперь мы можем обновить статическую переменную `ALLOCATOR` в модуле `allocator`, чтобы использовать наш новый `LinkedListAllocator`:

```rust
// src/allocator.rs

use linked_list::LinkedListAllocator;

#[global_allocator]
static ALLOCATOR: Locked<LinkedListAllocator> =
    Locked::new(LinkedListAllocator::new());
```

Поскольку функция `init` ведет себя одинаково для аллокаторов типа bump и linked list, нам не нужно изменять вызов `init` в `init_heap`.

Когда мы снова запускаем тесты `heap_allocation`, мы видим, что все тесты проходят успешно, включая тест `many_boxes_long_lived`, который не прошел с bump-аллокатором:

```
> cargo test --test heap_allocation
simple_allocation... [ok]
large_vec... [ok]
many_boxes... [ok]
many_boxes_long_lived... [ok]
```

Это показывает, что наш аллокатор связанного списка способен повторно использовать освобожденную память для последующих выделений.

### Discussion

В отличие от bump-аллокатора, аллокатор со связанным списком гораздо более подходит в качестве универсального аллокатора, главным образом потому, что он может напрямую повторно использовать освобожденную память. Однако у него есть и некоторые недостатки. Некоторые из них вызваны только нашей базовой реализацией, но есть и фундаментальные недостатки самой архитектуры аллокатора.

#### Merging Freed Blocks

Основная проблема нашей реализации заключается в том, что она только разбивает кучу на блоки, но никогда не объединяет их обратно. Рассмотрим следующий пример:

![](linked-list-allocator-fragmentation-on-dealloc.svg)

В первой строке на куче создаются три блока. Два из них снова освобождаются на второй строке, а третье — в третьей. Теперь вся куча снова не используется, но по-прежнему разделена на четыре отдельных блока. На этом этапе большое выделение может быть уже невозможно, поскольку ни один из четырех блоков не является достаточно большим. Со временем процесс продолжается, и куча разбивается на все более мелкие блоки. В какой-то момент куча становится настолько фрагментированной, что даже аллокация памяти нормального размера не удается.

Чтобы решить эту проблему, нам нужно объединить соседние освобожденные блоки. Для приведенного выше примера это будет означать следующее:

![](linked-list-allocator-merge-on-dealloc.svg)

Как и раньше, два из трех блоков освобождаются в строке `2`. Вместо того, чтобы сохранять фрагментированный кучу, мы теперь выполняем дополнительный шаг в строке `2a`, чтобы снова объединить два крайних правых блока. В строке `3` освобождается третья область (как и раньше), в результате чего получается полностью неиспользуемая куча, представленная тремя отдельными блоками. В дополнительном шаге объединения, на строке `3a`, мы снова объединяем три соседних блока.

Крейт `linked_list_allocator` реализует эту стратегию слияния следующий образом: вместо вставки освобожденных блоков в начало связанного списка при вызове `deallocate`, он всегда поддерживает список отсортированным по начальному адресу. Благодаря этому слияние можно выполнять непосредственно в процессе вызова `deallocate`, просто проанализировав адреса и размеры двух соседних блоков в списке. Конечно, операция деаллокации в этом случае выполняется медленнее, но это предотвращает фрагментацию кучи, о которой мы говорили выше.

#### Performance

Как мы уже выяснили, bump-аллокатор чрезвычайно быстр и может быть оптимизирован до нескольких ассемблерных инструкций. В этом отношении аллокатор на основе связанного списка работает значительно хуже. Проблема в том, что запрос на аллокацию может потребовать обхода всего связанного списка, пока не будет найден подходящий блок.

Поскольку длина списка зависит от количества неиспользуемых блоков памяти, производительность может чрезвычайно варьироваться в зависимости от программы. Программа, которая создаёт лишь нескольк блоков, будет демонстрировать относительно высокую производительность аллокаций. Однако для программы, которая сильно фрагментирует кучу множеством мелких блоков, производительность аллокаций окажется крайне низкой — ведь список станет очень длинным и будет в основном состоять из мелких, почти непригодных для повторного использования областей памяти.

Стоит отметить, что эта проблема производительности — не следствие нашей примитивной реализации, а фундаментальное ограничение самого подхода основанного на связанных списках. Поскольку производительность аллокаций имеет критическое значение для кода ядра, в следующем разделе мы рассмотрим третий дизайн аллокатора, который жертвует эффективностью использования памяти ради значительного повышения производительности.

## Fixed-Size Block Allocator

In the following, we present an allocator design that uses fixed-size memory blocks for fulfilling allocation requests. This way, the allocator often returns blocks that are larger than needed for allocations, which results in wasted memory due to [internal fragmentation]. On the other hand, it drastically reduces the time required to find a suitable block (compared to the linked list allocator), resulting in much better allocation performance.

### Introduction

The idea behind a _fixed-size block allocator_ is the following: Instead of allocating exactly as much memory as requested, we define a small number of block sizes and round up each allocation to the next block size. For example, with block sizes of 16, 64, and 512 bytes, an allocation of 4 bytes would return a 16-byte block, an allocation of 48 bytes a 64-byte block, and an allocation of 128 bytes a 512-byte block.

Like the linked list allocator, we keep track of the unused memory by creating a linked list in the unused memory. However, instead of using a single list with different block sizes, we create a separate list for each size class. Each list then only stores blocks of a single size. For example, with block sizes of 16, 64, and 512, there would be three separate linked lists in memory:

![](fixed-size-block-example.svg).

Instead of a single `head` pointer, we have the three head pointers `head_16`, `head_64`, and `head_512` that each point to the first unused block of the corresponding size. All nodes in a single list have the same size. For example, the list started by the `head_16` pointer only contains 16-byte blocks. This means that we no longer need to store the size in each list node since it is already specified by the name of the head pointer.

Since each element in a list has the same size, each list element is equally suitable for an allocation request. This means that we can very efficiently perform an allocation using the following steps:

- Round up the requested allocation size to the next block size. For example, when an allocation of 12 bytes is requested, we would choose the block size of 16 in the above example.
- Retrieve the head pointer for the list, e.g., for block size 16, we need to use `head_16`.
- Remove the first block from the list and return it.

Most notably, we can always return the first element of the list and no longer need to traverse the full list. Thus, allocations are much faster than with the linked list allocator.

#### Block Sizes and Wasted Memory

Depending on the block sizes, we lose a lot of memory by rounding up. For example, when a 512-byte block is returned for a 128-byte allocation, three-quarters of the allocated memory is unused. By defining reasonable block sizes, it is possible to limit the amount of wasted memory to some degree. For example, when using the powers of 2 (4, 8, 16, 32, 64, 128, …) as block sizes, we can limit the memory waste to half of the allocation size in the worst case and a quarter of the allocation size in the average case.

It is also common to optimize block sizes based on common allocation sizes in a program. For example, we could additionally add block size 24 to improve memory usage for programs that often perform allocations of 24 bytes. This way, the amount of wasted memory can often be reduced without losing the performance benefits.

#### Deallocation

Much like allocation, deallocation is also very performant. It involves the following steps:

- Round up the freed allocation size to the next block size. This is required since the compiler only passes the requested allocation size to `dealloc`, not the size of the block that was returned by `alloc`. By using the same size-adjustment function in both `alloc` and `dealloc`, we can make sure that we always free the correct amount of memory.
- Retrieve the head pointer for the list.
- Add the freed block to the front of the list by updating the head pointer.

Most notably, no traversal of the list is required for deallocation either. This means that the time required for a `dealloc` call stays the same regardless of the list length.

#### Fallback Allocator

Given that large allocations (>2&nbsp;KB) are often rare, especially in operating system kernels, it might make sense to fall back to a different allocator for these allocations. For example, we could fall back to a linked list allocator for allocations greater than 2048 bytes in order to reduce memory waste. Since only very few allocations of that size are expected, the linked list would stay small and the (de)allocations would still be reasonably fast.

#### Creating new Blocks

Above, we always assumed that there are always enough blocks of a specific size in the list to fulfill all allocation requests. However, at some point, the linked list for a given block size becomes empty. At this point, there are two ways we can create new unused blocks of a specific size to fulfill an allocation request:

- Allocate a new block from the fallback allocator (if there is one).
- Split a larger block from a different list. This best works if block sizes are powers of two. For example, a 32-byte block can be split into two 16-byte blocks.

For our implementation, we will allocate new blocks from the fallback allocator since the implementation is much simpler.

### Implementation

Now that we know how a fixed-size block allocator works, we can start our implementation. We won't depend on the implementation of the linked list allocator created in the previous section, so you can follow this part even if you skipped the linked list allocator implementation.

#### List Node

We start our implementation by creating a `ListNode` type in a new `allocator::fixed_size_block` module:

```rust
// in src/allocator.rs

pub mod fixed_size_block;
```

```rust
// in src/allocator/fixed_size_block.rs

struct ListNode {
    next: Option<&'static mut ListNode>,
}
```

This type is similar to the `ListNode` type of our [linked list allocator implementation], with the difference that we don't have a `size` field. It isn't needed because every block in a list has the same size with the fixed-size block allocator design.

[linked list allocator implementation]: #the-allocator-type

#### Block Sizes

Next, we define a constant `BLOCK_SIZES` slice with the block sizes used for our implementation:

```rust
// in src/allocator/fixed_size_block.rs

/// The block sizes to use.
///
/// The sizes must each be power of 2 because they are also used as
/// the block alignment (alignments must be always powers of 2).
const BLOCK_SIZES: &[usize] = &[8, 16, 32, 64, 128, 256, 512, 1024, 2048];
```

As block sizes, we use powers of 2, starting from 8 up to 2048. We don't define any block sizes smaller than 8 because each block must be capable of storing a 64-bit pointer to the next block when freed. For allocations greater than 2048 bytes, we will fall back to a linked list allocator.

To simplify the implementation, we define the size of a block as its required alignment in memory. So a 16-byte block is always aligned on a 16-byte boundary and a 512-byte block is aligned on a 512-byte boundary. Since alignments always need to be powers of 2, this rules out any other block sizes. If we need block sizes that are not powers of 2 in the future, we can still adjust our implementation for this (e.g., by defining a second `BLOCK_ALIGNMENTS` array).

#### The Allocator Type

Using the `ListNode` type and the `BLOCK_SIZES` slice, we can now define our allocator type:

```rust
// in src/allocator/fixed_size_block.rs

pub struct FixedSizeBlockAllocator {
    list_heads: [Option<&'static mut ListNode>; BLOCK_SIZES.len()],
    fallback_allocator: linked_list_allocator::Heap,
}
```

The `list_heads` field is an array of `head` pointers, one for each block size. This is implemented by using the `len()` of the `BLOCK_SIZES` slice as the array length. As a fallback allocator for allocations larger than the largest block size, we use the allocator provided by the `linked_list_allocator`. We could also use the `LinkedListAllocator` we implemented ourselves instead, but it has the disadvantage that it does not [merge freed blocks].

[merge freed blocks]: #merging-freed-blocks

For constructing a `FixedSizeBlockAllocator`, we provide the same `new` and `init` functions that we implemented for the other allocator types too:

```rust
// in src/allocator/fixed_size_block.rs

impl FixedSizeBlockAllocator {
    /// Creates an empty FixedSizeBlockAllocator.
    pub const fn new() -> Self {
        const EMPTY: Option<&'static mut ListNode> = None;
        FixedSizeBlockAllocator {
            list_heads: [EMPTY; BLOCK_SIZES.len()],
            fallback_allocator: linked_list_allocator::Heap::empty(),
        }
    }

    /// Initialize the allocator with the given heap bounds.
    ///
    /// This function is unsafe because the caller must guarantee that the given
    /// heap bounds are valid and that the heap is unused. This method must be
    /// called only once.
    pub unsafe fn init(&mut self, heap_start: usize, heap_size: usize) {
        unsafe { self.fallback_allocator.init(heap_start, heap_size); }
    }
}
```

The `new` function just initializes the `list_heads` array with empty nodes and creates an [`empty`] linked list allocator as `fallback_allocator`. The `EMPTY` constant is needed to tell the Rust compiler that we want to initialize the array with a constant value. Initializing the array directly as `[None; BLOCK_SIZES.len()]` does not work, because then the compiler requires `Option<&'static mut ListNode>` to implement the `Copy` trait, which it does not. This is a current limitation of the Rust compiler, which might go away in the future.

[`empty`]: https://docs.rs/linked_list_allocator/0.9.0/linked_list_allocator/struct.Heap.html#method.empty

The unsafe `init` function only calls the [`init`] function of the `fallback_allocator` without doing any additional initialization of the `list_heads` array. Instead, we will initialize the lists lazily on `alloc` and `dealloc` calls.

[`init`]: https://docs.rs/linked_list_allocator/0.9.0/linked_list_allocator/struct.Heap.html#method.init

For convenience, we also create a private `fallback_alloc` method that allocates using the `fallback_allocator`:

```rust
// in src/allocator/fixed_size_block.rs

use alloc::alloc::Layout;
use core::ptr;

impl FixedSizeBlockAllocator {
    /// Allocates using the fallback allocator.
    fn fallback_alloc(&mut self, layout: Layout) -> *mut u8 {
        match self.fallback_allocator.allocate_first_fit(layout) {
            Ok(ptr) => ptr.as_ptr(),
            Err(_) => ptr::null_mut(),
        }
    }
}
```

The [`Heap`] type of the `linked_list_allocator` crate does not implement [`GlobalAlloc`] (as it's [not possible without locking]). Instead, it provides an [`allocate_first_fit`] method that has a slightly different interface. Instead of returning a `*mut u8` and using a null pointer to signal an error, it returns a `Result<NonNull<u8>, ()>`. The [`NonNull`] type is an abstraction for a raw pointer that is guaranteed to not be a null pointer. By mapping the `Ok` case to the [`NonNull::as_ptr`] method and the `Err` case to a null pointer, we can easily translate this back to a `*mut u8` type.

[`Heap`]: https://docs.rs/linked_list_allocator/0.9.0/linked_list_allocator/struct.Heap.html
[not possible without locking]: #globalalloc-and-mutability
[`allocate_first_fit`]: https://docs.rs/linked_list_allocator/0.9.0/linked_list_allocator/struct.Heap.html#method.allocate_first_fit
[`NonNull`]: https://doc.rust-lang.org/nightly/core/ptr/struct.NonNull.html
[`NonNull::as_ptr`]: https://doc.rust-lang.org/nightly/core/ptr/struct.NonNull.html#method.as_ptr

#### Calculating the List Index

Before we implement the `GlobalAlloc` trait, we define a `list_index` helper function that returns the lowest possible block size for a given [`Layout`]:

```rust
// in src/allocator/fixed_size_block.rs

/// Choose an appropriate block size for the given layout.
///
/// Returns an index into the `BLOCK_SIZES` array.
fn list_index(layout: &Layout) -> Option<usize> {
    let required_block_size = layout.size().max(layout.align());
    BLOCK_SIZES.iter().position(|&s| s >= required_block_size)
}
```

The block must have at least the size and alignment required by the given `Layout`. Since we defined that the block size is also its alignment, this means that the `required_block_size` is the [maximum] of the layout's [`size()`] and [`align()`] attributes. To find the next-larger block in the `BLOCK_SIZES` slice, we first use the [`iter()`] method to get an iterator and then the [`position()`] method to find the index of the first block that is at least as large as the `required_block_size`.

[maximum]: https://doc.rust-lang.org/core/cmp/trait.Ord.html#method.max
[`size()`]: https://doc.rust-lang.org/core/alloc/struct.Layout.html#method.size
[`align()`]: https://doc.rust-lang.org/core/alloc/struct.Layout.html#method.align
[`iter()`]: https://doc.rust-lang.org/std/primitive.slice.html#method.iter
[`position()`]:  https://doc.rust-lang.org/core/iter/trait.Iterator.html#method.position

Note that we don't return the block size itself, but the index into the `BLOCK_SIZES` slice. The reason is that we want to use the returned index as an index into the `list_heads` array.

#### Implementing `GlobalAlloc`

The last step is to implement the `GlobalAlloc` trait:

```rust
// in src/allocator/fixed_size_block.rs

use super::Locked;
use alloc::alloc::GlobalAlloc;

unsafe impl GlobalAlloc for Locked<FixedSizeBlockAllocator> {
    unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
        todo!();
    }

    unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
        todo!();
    }
}
```

Like for the other allocators, we don't implement the `GlobalAlloc` trait directly for our allocator type, but use the [`Locked` wrapper] to add synchronized interior mutability. Since the `alloc` and `dealloc` implementations are relatively large, we introduce them one by one in the following.

##### `alloc`

The implementation of the `alloc` method looks like this:

```rust
// in `impl` block in src/allocator/fixed_size_block.rs

unsafe fn alloc(&self, layout: Layout) -> *mut u8 {
    let mut allocator = self.lock();
    match list_index(&layout) {
        Some(index) => {
            match allocator.list_heads[index].take() {
                Some(node) => {
                    allocator.list_heads[index] = node.next.take();
                    node as *mut ListNode as *mut u8
                }
                None => {
                    // no block exists in list => allocate new block
                    let block_size = BLOCK_SIZES[index];
                    // only works if all block sizes are a power of 2
                    let block_align = block_size;
                    let layout = Layout::from_size_align(block_size, block_align)
                        .unwrap();
                    allocator.fallback_alloc(layout)
                }
            }
        }
        None => allocator.fallback_alloc(layout),
    }
}
```

Let's go through it step by step:

First, we use the `Locked::lock` method to get a mutable reference to the wrapped allocator instance. Next, we call the `list_index` function we just defined to calculate the appropriate block size for the given layout and get the corresponding index into the `list_heads` array. If this index is `None`, no block size fits for the allocation, therefore we use the `fallback_allocator` using the `fallback_alloc` function.

If the list index is `Some`, we try to remove the first node in the corresponding list started by `list_heads[index]` using the [`Option::take`] method. If the list is not empty, we enter the `Some(node)` branch of the `match` statement, where we point the head pointer of the list to the successor of the popped `node` (by using [`take`][`Option::take`] again). Finally, we return the popped `node` pointer as a `*mut u8`.

[`Option::take`]: https://doc.rust-lang.org/core/option/enum.Option.html#method.take

If the list head is `None`, it indicates that the list of blocks is empty. This means that we need to construct a new block as [described above](#creating-new-blocks). For that, we first get the current block size from the `BLOCK_SIZES` slice and use it as both the size and the alignment for the new block. Then we create a new `Layout` from it and call the `fallback_alloc` method to perform the allocation. The reason for adjusting the layout and alignment is that the block will be added to the block list on deallocation.

#### `dealloc`

The implementation of the `dealloc` method looks like this:

```rust
// in src/allocator/fixed_size_block.rs

use core::{mem, ptr::NonNull};

// inside the `unsafe impl GlobalAlloc` block

unsafe fn dealloc(&self, ptr: *mut u8, layout: Layout) {
    let mut allocator = self.lock();
    match list_index(&layout) {
        Some(index) => {
            let new_node = ListNode {
                next: allocator.list_heads[index].take(),
            };
            // verify that block has size and alignment required for storing node
            assert!(mem::size_of::<ListNode>() <= BLOCK_SIZES[index]);
            assert!(mem::align_of::<ListNode>() <= BLOCK_SIZES[index]);
            let new_node_ptr = ptr as *mut ListNode;
            unsafe {
                new_node_ptr.write(new_node);
                allocator.list_heads[index] = Some(&mut *new_node_ptr);
            }
        }
        None => {
            let ptr = NonNull::new(ptr).unwrap();
            unsafe {
                allocator.fallback_allocator.deallocate(ptr, layout);
            }
        }
    }
}
```

Like in `alloc`, we first use the `lock` method to get a mutable allocator reference and then the `list_index` function to get the block list corresponding to the given `Layout`. If the index is `None`, no fitting block size exists in `BLOCK_SIZES`, which indicates that the allocation was created by the fallback allocator. Therefore, we use its [`deallocate`][`Heap::deallocate`] to free the memory again. The method expects a [`NonNull`] instead of a `*mut u8`, so we need to convert the pointer first. (The `unwrap` call only fails when the pointer is null, which should never happen when the compiler calls `dealloc`.)

[`Heap::deallocate`]: https://docs.rs/linked_list_allocator/0.9.0/linked_list_allocator/struct.Heap.html#method.deallocate

If `list_index` returns a block index, we need to add the freed memory block to the list. For that, we first create a new `ListNode` that points to the current list head (by using [`Option::take`] again). Before we write the new node into the freed memory block, we first assert that the current block size specified by `index` has the required size and alignment for storing a `ListNode`. Then we perform the write by converting the given `*mut u8` pointer to a `*mut ListNode` pointer and then calling the unsafe [`write`][`pointer::write`] method on it. The last step is to set the head pointer of the list, which is currently `None` since we called `take` on it, to our newly written `ListNode`. For that, we convert the raw `new_node_ptr` to a mutable reference.

[`pointer::write`]: https://doc.rust-lang.org/std/primitive.pointer.html#method.write

There are a few things worth noting:

- We don't differentiate between blocks allocated from a block list and blocks allocated from the fallback allocator. This means that new blocks created in `alloc` are added to the block list on `dealloc`, thereby increasing the number of blocks of that size.
- The `alloc` method is the only place where new blocks are created in our implementation. This means that we initially start with empty block lists and only fill these lists lazily when allocations of their block size are performed.
- We don't need `unsafe` blocks in `alloc` and `dealloc`, even though we perform some `unsafe` operations. The reason is that Rust currently treats the complete body of unsafe functions as one large `unsafe` block. Since using explicit `unsafe` blocks has the advantage that it's obvious which operations are unsafe and which are not, there is a [proposed RFC](https://github.com/rust-lang/rfcs/pull/2585) to change this behavior.

### Using it

To use our new `FixedSizeBlockAllocator`, we need to update the `ALLOCATOR` static in the `allocator` module:

```rust
// in src/allocator.rs

use fixed_size_block::FixedSizeBlockAllocator;

#[global_allocator]
static ALLOCATOR: Locked<FixedSizeBlockAllocator> = Locked::new(
    FixedSizeBlockAllocator::new());
```

Since the `init` function behaves the same for all allocators we implemented, we don't need to modify the `init` call in `init_heap`.

When we now run our `heap_allocation` tests again, all tests should still pass:

```
> cargo test --test heap_allocation
simple_allocation... [ok]
large_vec... [ok]
many_boxes... [ok]
many_boxes_long_lived... [ok]
```

Our new allocator seems to work!

### Discussion

While the fixed-size block approach has much better performance than the linked list approach, it wastes up to half of the memory when using powers of 2 as block sizes. Whether this tradeoff is worth it heavily depends on the application type. For an operating system kernel, where performance is critical, the fixed-size block approach seems to be the better choice.

On the implementation side, there are various things that we could improve in our current implementation:

- Instead of only allocating blocks lazily using the fallback allocator, it might be better to pre-fill the lists to improve the performance of initial allocations.
- To simplify the implementation, we only allowed block sizes that are powers of 2 so that we could also use them as the block alignment. By storing (or calculating) the alignment in a different way, we could also allow arbitrary other block sizes. This way, we could add more block sizes, e.g., for common allocation sizes, in order to minimize the wasted memory.
- We currently only create new blocks, but never free them again. This results in fragmentation and might eventually result in allocation failure for large allocations. It might make sense to enforce a maximum list length for each block size. When the maximum length is reached, subsequent deallocations are freed using the fallback allocator instead of being added to the list.
- Instead of falling back to a linked list allocator, we could have a special allocator for allocations greater than 4&nbsp;KiB. The idea is to utilize [paging], which operates on 4&nbsp;KiB pages, to map a continuous block of virtual memory to non-continuous physical frames. This way, fragmentation of unused memory is no longer a problem for large allocations.
- With such a page allocator, it might make sense to add block sizes up to 4&nbsp;KiB and drop the linked list allocator completely. The main advantages of this would be reduced fragmentation and improved performance predictability, i.e., better worst-case performance.

[paging]: @/edition-2/posts/08-paging-introduction/index.md

It's important to note that the implementation improvements outlined above are only suggestions. Allocators used in operating system kernels are typically highly optimized for the specific workload of the kernel, which is only possible through extensive profiling.

### Variations

There are also many variations of the fixed-size block allocator design. Two popular examples are the _slab allocator_ and the _buddy allocator_, which are also used in popular kernels such as Linux. In the following, we give a short introduction to these two designs.

#### Slab Allocator

The idea behind a [slab allocator] is to use block sizes that directly correspond to selected types in the kernel. This way, allocations of those types fit a block size exactly and no memory is wasted. Sometimes, it might be even possible to preinitialize type instances in unused blocks to further improve performance.

[slab allocator]: https://en.wikipedia.org/wiki/Slab_allocation

Slab allocation is often combined with other allocators. For example, it can be used together with a fixed-size block allocator to further split an allocated block in order to reduce memory waste. It is also often used to implement an [object pool pattern] on top of a single large allocation.

[object pool pattern]: https://en.wikipedia.org/wiki/Object_pool_pattern

#### Buddy Allocator

Instead of using a linked list to manage freed blocks, the [buddy allocator] design uses a [binary tree] data structure together with power-of-2 block sizes. When a new block of a certain size is required, it splits a larger sized block into two halves, thereby creating two child nodes in the tree. Whenever a block is freed again, its neighbor block in the tree is analyzed. If the neighbor is also free, the two blocks are joined back together to form a block of twice the size.

The advantage of this merge process is that [external fragmentation] is reduced so that small freed blocks can be reused for a large allocation. It also does not use a fallback allocator, so the performance is more predictable. The biggest drawback is that only power-of-2 block sizes are possible, which might result in a large amount of wasted memory due to [internal fragmentation]. For this reason, buddy allocators are often combined with a slab allocator to further split an allocated block into multiple smaller blocks.

[buddy allocator]: https://en.wikipedia.org/wiki/Buddy_memory_allocation
[binary tree]: https://en.wikipedia.org/wiki/Binary_tree
[external fragmentation]: https://en.wikipedia.org/wiki/Fragmentation_(computing)#External_fragmentation
[internal fragmentation]: https://en.wikipedia.org/wiki/Fragmentation_(computing)#Internal_fragmentation


## Summary

This post gave an overview of different allocator designs. We learned how to implement a basic [bump allocator], which hands out memory linearly by increasing a single `next` pointer. While bump allocation is very fast, it can only reuse memory after all allocations have been freed. For this reason, it is rarely used as a global allocator.

[bump allocator]: @/edition-2/posts/11-allocator-designs/index.md#bump-allocator

Next, we created a [linked list allocator] that uses the freed memory blocks itself to create a linked list, the so-called [free list]. This list makes it possible to store an arbitrary number of freed blocks of different sizes. While no memory waste occurs, the approach suffers from poor performance because an allocation request might require a complete traversal of the list. Our implementation also suffers from [external fragmentation] because it does not merge adjacent freed blocks back together.

[linked list allocator]: @/edition-2/posts/11-allocator-designs/index.md#linked-list-allocator
[free list]: https://en.wikipedia.org/wiki/Free_list

To fix the performance problems of the linked list approach, we created a [fixed-size block allocator] that predefines a fixed set of block sizes. For each block size, a separate [free list] exists so that allocations and deallocations only need to insert/pop at the front of the list and are thus very fast. Since each allocation is rounded up to the next larger block size, some memory is wasted due to [internal fragmentation].

[fixed-size block allocator]: @/edition-2/posts/11-allocator-designs/index.md#fixed-size-block-allocator

There are many more allocator designs with different tradeoffs. [Slab allocation] works well to optimize the allocation of common fixed-size structures, but is not applicable in all situations. [Buddy allocation] uses a binary tree to merge freed blocks back together, but wastes a large amount of memory because it only supports power-of-2 block sizes. It's also important to remember that each kernel implementation has a unique workload, so there is no "best" allocator design that fits all cases.

[Slab allocation]: @/edition-2/posts/11-allocator-designs/index.md#slab-allocator
[Buddy allocation]: @/edition-2/posts/11-allocator-designs/index.md#buddy-allocator


## What's next?

With this post, we conclude our memory management implementation for now. Next, we will start exploring [_multitasking_], starting with cooperative multitasking in the form of [_async/await_]. In subsequent posts, we will then explore [_threads_], [_multiprocessing_], and [_processes_].

[_multitasking_]: https://en.wikipedia.org/wiki/Computer_multitasking
[_threads_]: https://en.wikipedia.org/wiki/Thread_(computing)
[_processes_]: https://en.wikipedia.org/wiki/Process_(computing)
[_multiprocessing_]: https://en.wikipedia.org/wiki/Multiprocessing
[_async/await_]: https://rust-lang.github.io/async-book/01_getting_started/04_async_await_primer.html
